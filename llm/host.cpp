//=============================================================================
// Auto generated by Allo
//=============================================================================

// OpenCL utility layer include
#include "xcl2.hpp"
#include <ap_int.h>
#include <algorithm>
#include <cstdio>
#include <random>
#include <vector>
#include <iomanip>

// HBM channels
#define MAX_HBM_CHANNEL_COUNT 32
#define CHANNEL_NAME(n) n | XCL_MEM_TOPOLOGY
const int HBM[MAX_HBM_CHANNEL_COUNT] = {
    CHANNEL_NAME(0),  CHANNEL_NAME(1),  CHANNEL_NAME(2),  CHANNEL_NAME(3),  CHANNEL_NAME(4),
    CHANNEL_NAME(5),  CHANNEL_NAME(6),  CHANNEL_NAME(7),  CHANNEL_NAME(8),  CHANNEL_NAME(9),
    CHANNEL_NAME(10), CHANNEL_NAME(11), CHANNEL_NAME(12), CHANNEL_NAME(13), CHANNEL_NAME(14),
    CHANNEL_NAME(15), CHANNEL_NAME(16), CHANNEL_NAME(17), CHANNEL_NAME(18), CHANNEL_NAME(19),
    CHANNEL_NAME(20), CHANNEL_NAME(21), CHANNEL_NAME(22), CHANNEL_NAME(23), CHANNEL_NAME(24),
    CHANNEL_NAME(25), CHANNEL_NAME(26), CHANNEL_NAME(27), CHANNEL_NAME(28), CHANNEL_NAME(29),
    CHANNEL_NAME(30), CHANNEL_NAME(31)};

const int DDR[2] = {CHANNEL_NAME(32), CHANNEL_NAME(33)};

int main(int argc, char** argv) {
    if (argc != 2) {
        std::cout << "Usage: " << argv[0] << " <XCLBIN File>" << std::endl;
        return EXIT_FAILURE;
    }

    std::string binaryFile = argv[1];
    cl_int err;
    cl::CommandQueue q;
    cl::Context context;
    cl::Program program;
    cl::Kernel krnl_GPT_layer_dataflow_region_1;
    cl::Kernel krnl_GPT_layer_dataflow_region_2;
    cl::Kernel krnl_GPT_layer_dataflow_region_3;
    // Allocate Memory in Host Memory
    // When creating a buffer with user pointer (CL_MEM_USE_HOST_PTR), under the
    // hood user ptr is used if it is properly aligned. when not aligned, runtime had no choice
    // but to create its own host side buffer. So it is recommended to use this allocator if
    // user wish to create buffer using CL_MEM_USE_HOST_PTR to align user buffer to page
    // boundary. It will ensure that user buffer is used when user create Buffer/Mem object with
    // CL_MEM_USE_HOST_PTR
    size_t size_bytes_in0 = sizeof(ap_uint<256>) * 16*1024;
    std::vector<ap_uint<256>, aligned_allocator<ap_uint<256> > > source_in0(16*1024);
    
    size_t size_bytes_wk = sizeof(ap_uint<128>) * 32*1024;
    std::vector<ap_uint<128>, aligned_allocator<ap_uint<128> > > source_wk(32*1024);
    size_t size_bytes_wv = sizeof(ap_uint<128>) * 32*1024;
    std::vector<ap_uint<128>, aligned_allocator<ap_uint<128> > > source_wv(32*1024);
    size_t size_bytes_wq = sizeof(ap_uint<128>) * 32*1024;
    std::vector<ap_uint<128>, aligned_allocator<ap_uint<128> > > source_wq(32*1024);

    size_t size_bytes_w_ds0 = sizeof(ap_uint<128>) * 32*1024;
    std::vector<ap_uint<128>, aligned_allocator<ap_uint<128> > > source_w_ds0(32*1024);
    size_t size_bytes_w_ds1 = sizeof(ap_uint<128>) * 128*1024;
    std::vector<ap_uint<128>, aligned_allocator<ap_uint<128> > > source_w_ds1(128*1024);
    size_t size_bytes_w_ds2 = sizeof(ap_uint<128>) * 32*4096;
    std::vector<ap_uint<128>, aligned_allocator<ap_uint<128> > > source_w_ds2(32*4096);

    size_t size_bytes_out0 = sizeof(ap_uint<256>) * 16*1024;
    std::vector<ap_uint<256>, aligned_allocator<ap_uint<256> > > source_out0(16*1024);

    // OPENCL HOST CODE AREA START
    // get_xil_devices() is a utility API which will find the xilinx
    // platforms and will return list of devices connected to Xilinx platform
    auto devices = xcl::get_xil_devices();
    // read_binary_file() is a utility API which will load the binaryFile
    // and will return the pointer to file buffer.
    auto fileBuf = xcl::read_binary_file(binaryFile);
    cl::Program::Binaries bins{{fileBuf.data(), fileBuf.size()}};
    bool valid_device = false;
    for (unsigned int i = 0; i < devices.size(); i++) {
        auto device = devices[i];
        // Creating Context and Command Queue for selected Device
        OCL_CHECK(err, context = cl::Context(device, nullptr, nullptr, nullptr, &err));
        OCL_CHECK(err, q = cl::CommandQueue(context, device, 
        CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE | // dataflow
        CL_QUEUE_PROFILING_ENABLE, &err));
        std::cout << "Trying to program device[" << i << "]: " << device.getInfo<CL_DEVICE_NAME>() << std::endl;
        cl::Program program(context, {device}, bins, nullptr, &err);
        if (err != CL_SUCCESS) {
            std::cout << "Failed to program device[" << i << "] with xclbin file!\n";
        } else {
            std::cout << "Device[" << i << "]: program successful!\n";
            OCL_CHECK(err, krnl_GPT_layer_dataflow_region_1 = cl::Kernel(program, "GPT_layer_dataflow_region_1", &err));
            OCL_CHECK(err, krnl_GPT_layer_dataflow_region_2 = cl::Kernel(program, "GPT_layer_dataflow_region_2", &err));
            OCL_CHECK(err, krnl_GPT_layer_dataflow_region_3 = cl::Kernel(program, "GPT_layer_dataflow_region_3", &err));
            valid_device = true;
            break; // we break because we found a valid device
        }
    }
    if (!valid_device) {
        std::cout << "Failed to program any device found, exit!\n";
        exit(EXIT_FAILURE);
    }
    // Allocate Buffer in Global Memory
    // Buffers are allocated using CL_MEM_USE_HOST_PTR for efficient memory and
    // Device-to-host communication
    cl_mem_ext_ptr_t source_in0_hbm;
    source_in0_hbm.obj = source_in0.data();
    source_in0_hbm.param = 0;
    source_in0_hbm.flags = HBM[0];

    cl_mem_ext_ptr_t source_wk_hbm;
    source_wk_hbm.obj = source_wk.data();
    source_wk_hbm.param = 0;
    source_wk_hbm.flags = HBM[1];

    cl_mem_ext_ptr_t source_wv_hbm;
    source_wv_hbm.obj = source_wv.data();
    source_wv_hbm.param = 0;
    source_wv_hbm.flags = HBM[2];

    cl_mem_ext_ptr_t source_wq_hbm;
    source_wq_hbm.obj = source_wq.data();
    source_wq_hbm.param = 0;
    source_wq_hbm.flags = HBM[3];

    cl_mem_ext_ptr_t source_w_ds0_hbm;
    source_w_ds0_hbm.obj = source_w_ds0.data();
    source_w_ds0_hbm.param = 0;
    source_w_ds0_hbm.flags = HBM[4];

    cl_mem_ext_ptr_t source_w_ds1_hbm;
    source_w_ds1_hbm.obj = source_w_ds1.data();
    source_w_ds1_hbm.param = 0;
    source_w_ds1_hbm.flags = HBM[5];

    cl_mem_ext_ptr_t source_w_ds2_hbm;
    source_w_ds2_hbm.obj = source_w_ds2.data();
    source_w_ds2_hbm.param = 0;
    source_w_ds2_hbm.flags = HBM[6];

    OCL_CHECK(err, cl::Buffer buffer_in0(context, CL_MEM_EXT_PTR_XILINX | CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY, size_bytes_in0, &source_in0_hbm, &err));
    OCL_CHECK(err, cl::Buffer buffer_wk(context, CL_MEM_EXT_PTR_XILINX | CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY, size_bytes_wk, &source_wk_hbm, &err));
    OCL_CHECK(err, cl::Buffer buffer_wv(context, CL_MEM_EXT_PTR_XILINX | CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY, size_bytes_wv, &source_wv_hbm, &err));
    OCL_CHECK(err, cl::Buffer buffer_wq(context, CL_MEM_EXT_PTR_XILINX | CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY, size_bytes_wq, &source_wq_hbm, &err));
    OCL_CHECK(err, cl::Buffer buffer_w_ds0(context, CL_MEM_EXT_PTR_XILINX | CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY, size_bytes_w_ds0, &source_w_ds0_hbm, &err));
    OCL_CHECK(err, cl::Buffer buffer_w_ds1(context, CL_MEM_EXT_PTR_XILINX | CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY, size_bytes_w_ds1, &source_w_ds1_hbm, &err));
    OCL_CHECK(err, cl::Buffer buffer_w_ds2(context, CL_MEM_EXT_PTR_XILINX | CL_MEM_USE_HOST_PTR | CL_MEM_READ_ONLY, size_bytes_w_ds2, &source_w_ds2_hbm, &err));
    OCL_CHECK(err, cl::Buffer buffer_out0(context, CL_MEM_USE_HOST_PTR | CL_MEM_WRITE_ONLY, size_bytes_out0, source_out0.data(), &err));

    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_1.setArg(0, buffer_in0));
    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_1.setArg(1, buffer_in0));
    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_1.setArg(2, buffer_in0));
    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_1.setArg(3, buffer_wk));
    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_1.setArg(4, buffer_wv));
    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_1.setArg(5, buffer_wq));

    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_2.setArg(0, buffer_w_ds0));

    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_3.setArg(0, buffer_w_ds1));
    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_3.setArg(1, buffer_w_ds2));
    OCL_CHECK(err, err = krnl_GPT_layer_dataflow_region_3.setArg(3, buffer_out0));
    
    cl::Event event;
    uint64_t nstimestart, nstimeend;
    uint64_t exe_time = 0;

    //single layer
    // Copy input data to device global memory
    OCL_CHECK(err, err = q.enqueueMigrateMemObjects({buffer_in0}, 0 /* 0 means from host*/));
    q.finish();

    std::cout << "|-------------------------+-------------------------|\n"
              << "| Single Layer            |    Wall-Clock Time (ns) |\n"
              << "|-------------------------+-------------------------|\n";

    // Launch the Kernel
    OCL_CHECK(err, err = q.enqueueTask(krnl_GPT_layer_dataflow_region_1, nullptr, &event));
    OCL_CHECK(err, err = q.enqueueTask(krnl_GPT_layer_dataflow_region_2, nullptr, &event));
    OCL_CHECK(err, err = q.enqueueTask(krnl_GPT_layer_dataflow_region_3, nullptr, &event));
    q.finish();

    // Copy Result from Device Global Memory to Host Local Memory
    OCL_CHECK(err, err = q.enqueueMigrateMemObjects({buffer_out0}, CL_MIGRATE_MEM_OBJECT_HOST));
    q.finish();
    // OpenCL Host Code Ends

    // Get the execution time
    OCL_CHECK(err, err = event.getProfilingInfo<uint64_t>(CL_PROFILING_COMMAND_START, &nstimestart));
    OCL_CHECK(err, err = event.getProfilingInfo<uint64_t>(CL_PROFILING_COMMAND_END, &nstimeend));
    exe_time = nstimeend - nstimestart;
    std::cout << "| " << std::left << std::setw(24) << "GPT_layer: "
              << "|" << std::right << std::setw(24) << exe_time << " |\n";
    std::cout << "|-------------------------+-------------------------|\n\n";


    //multi layer
    // Copy input data to device global memory
    OCL_CHECK(err, err = q.enqueueMigrateMemObjects({buffer_in0}, 0 /* 0 means from host*/));
    q.finish();

    std::cout << "|-------------------------+-------------------------|\n"
              << "| Multi Layer             |    Wall-Clock Time (ns) |\n"
              << "|-------------------------+-------------------------|\n";

    // Launch the Kernel
    exe_time = 0;
    for(int i = 0; i < 24; i++){
        OCL_CHECK(err, err = q.enqueueTask(krnl_GPT_layer_dataflow_region_1, nullptr, &event));
        OCL_CHECK(err, err = q.enqueueTask(krnl_GPT_layer_dataflow_region_2, nullptr, &event));
        OCL_CHECK(err, err = q.enqueueTask(krnl_GPT_layer_dataflow_region_3, nullptr, &event));
        q.finish();
        OCL_CHECK(err, err = event.getProfilingInfo<uint64_t>(CL_PROFILING_COMMAND_START, &nstimestart));
        OCL_CHECK(err, err = event.getProfilingInfo<uint64_t>(CL_PROFILING_COMMAND_END, &nstimeend));
        exe_time += nstimeend - nstimestart;
    }

    // Copy Result from Device Global Memory to Host Local Memory
    OCL_CHECK(err, err = q.enqueueMigrateMemObjects({buffer_out0}, CL_MIGRATE_MEM_OBJECT_HOST));
    q.finish();
    // OpenCL Host Code Ends

    // Get the execution time
    std::cout << "| " << std::left << std::setw(24) << "GPT_layer: "
              << "|" << std::right << std::setw(24) << exe_time << " |\n";
    std::cout << "|-------------------------+-------------------------|\n";
    std::cout << "Note: Wall Clock Time is meaningful for real hardware execution "
              << "only, not for emulation.\n";
    std::cout << "Please refer to profile summary for kernel execution time for "
              << "hardware emulation.\n";
    std::cout << "TEST PASSED\n\n";
    return EXIT_SUCCESS;
}